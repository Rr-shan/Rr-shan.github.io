<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="srf之论文阅读A3NCF"><meta name="keywords" content="srf"><meta name="author" content="Rr-shan,undefined"><meta name="copyright" content="Rr-shan"><title>srf之论文阅读A3NCF【Sanzzi】</title><link rel="stylesheet" href="../../../../css/fan.css"><link rel="stylesheet" href="../../../../css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="icon" href="../../../../favicon.ico"><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="../../../../js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
}</script></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="toggle-sidebar-info button-hover"><span data-toggle="文章目录">站点概览</span></div><div class="sidebar-toc"><div class="sidebar-toc-title">目录</div><div class="sidebar-toc-progress"><span class="progress-notice">您已阅读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc-progress-bar"></div></div><div class="sidebar-toc-content" id="sidebar-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#A3NCF-An-Adaptive-Aspect-Attention-Model-for-Rating-Prediction"><span class="toc-number">1.</span> <span class="toc-text">A3NCF: An Adaptive Aspect Attention Model for Rating Prediction</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-为什么要讲这个-amp-传统方法有什么问题-（对比）"><span class="toc-number">1.1.</span> <span class="toc-text">1. 为什么要讲这个?&amp; 传统方法有什么问题?（对比）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-提出了什么方法-有什么创新点"><span class="toc-number">1.2.</span> <span class="toc-text">2. 提出了什么方法 ? 有什么创新点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-原理"><span class="toc-number">1.3.</span> <span class="toc-text">3. 原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Input模块"><span class="toc-number">1.3.1.</span> <span class="toc-text">1. Input模块</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#对应算法："><span class="toc-number">1.3.1.1.</span> <span class="toc-text">对应算法：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Feature-Fusion"><span class="toc-number">1.3.2.</span> <span class="toc-text">2. Feature Fusion</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Attentive-Interaction（最核心）"><span class="toc-number">1.3.3.</span> <span class="toc-text">3. Attentive Interaction（最核心）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#我们为了得到F："><span class="toc-number">1.3.3.1.</span> <span class="toc-text">我们为了得到F：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#interact-att-weights-user-embed-item-embed"><span class="toc-number">1.4.</span> <span class="toc-text">interact = att_weights * user_embed * item_embed</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#怎么得到a"><span class="toc-number">1.4.0.1.</span> <span class="toc-text">怎么得到a</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Rating-Prediction"><span class="toc-number">1.4.1.</span> <span class="toc-text">4. Rating Prediction</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-损失函数-amp-优化器"><span class="toc-number">1.5.</span> <span class="toc-text">4. 损失函数&amp;优化器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#损失函数："><span class="toc-number">1.5.1.</span> <span class="toc-text">损失函数：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#optimization："><span class="toc-number">1.5.2.</span> <span class="toc-text">optimization：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-数据集是怎么划分的"><span class="toc-number">1.6.</span> <span class="toc-text">5. 数据集是怎么划分的?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-实验的结果："><span class="toc-number">1.7.</span> <span class="toc-text">6. 实验的结果：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#疑问："><span class="toc-number">1.8.</span> <span class="toc-text">疑问：</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-这里面打开为什么什么都没有呀？"><span class="toc-number">1.8.0.0.1.</span> <span class="toc-text">1. 这里面打开为什么什么都没有呀？</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-具体的怎么运用到现在的模型当中"><span class="toc-number">1.8.0.0.2.</span> <span class="toc-text">2. 具体的怎么运用到现在的模型当中</span></a></li></ol></li></ol><li class="toc-item toc-level-2"><a class="toc-link" href="#参考："><span class="toc-number">1.9.</span> <span class="toc-text">参考：</span></a></li></div></div><div class="author-info hide"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">Rr-shan</div><div class="author-info-description"></div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/Rr-shan" target="_blank">GitHub<i class="icon-dot bg-color5"></i></a><a class="links-button button-hover" href="mailto:1224559633@qq.com" target="_blank">E-Mail<i class="icon-dot bg-color3"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="../../../../archives"><span class="pull-top">日志</span><span class="pull-bottom">47</span></a><a class="author-info-articles-tags article-meta" href="../../../../tags"><span class="pull-top">标签</span><span class="pull-bottom">12</span></a><a class="author-info-articles-categories article-meta" href="../../../../categories"><span class="pull-top">分类</span><span class="pull-bottom">12</span></a></div><div class="friend-link"><a class="friend-link-text" href="https://chao-yin-github.github.io/" target="_blank">yccccc~~~~~~~~</a><a class="friend-link-text" href="https://sanzzi.cn" target="_blank">Mywebsite</a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="title-name" href="/">Sanzzi</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><article id="post"><div class="post-header"><div class="title">srf之论文阅读A3NCF</div><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 发表于 2020-04-14 | 更新于 2020-04-14</time><!--time.button-hover.post-date #[i.fas.fa-calendar-alt.article-icon(aria-hidden="true")] #[=__('post.modified')] #[=date(page['updated'], config.date_format)]--><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="../../../../categories/DeepLearning/">DeepLearning</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../../../tags/srf/">srf</a></div></div></div><div class="main-content"><h1 id="A3NCF-An-Adaptive-Aspect-Attention-Model-for-Rating-Prediction"><a href="#A3NCF-An-Adaptive-Aspect-Attention-Model-for-Rating-Prediction" class="headerlink" title="A3NCF: An Adaptive Aspect Attention Model for Rating Prediction"></a>A3NCF: An Adaptive Aspect Attention Model for Rating Prediction</h1><hr>
<h2 id="1-为什么要讲这个-amp-传统方法有什么问题-（对比）"><a href="#1-为什么要讲这个-amp-传统方法有什么问题-（对比）" class="headerlink" title="1. 为什么要讲这个?&amp; 传统方法有什么问题?（对比）"></a>1. 为什么要讲这个?&amp; 传统方法有什么问题?（对比）</h2><ul>
<li>提出了<strong>矩阵分解</strong>的一些不足之处，用户对物品的一个rate只是用户对其的一个<strong>总体评价</strong>，<em>缺少可解释性</em>。<blockquote>
<p>例如：一个用户对一部手机有较高的评分可能只是因为它的摄像头好或者是电池的续航能力强，但是这并不代表对这部手机的整体有了一个很高的评价。所以说MF并不能实现<strong>细粒度的建模。</strong></p>
</blockquote>
</li>
<li>基于这个限制，研究者们将目光转向了<strong>文本评论</strong>，评论中包含了很多丰富的信息，由于深度学习有着很强的提取特征能力，研究者们用它来学习评论中<strong>用户的喜好</strong>和<strong>物品的特征</strong>。虽然这些方法比起MF有着更好的性能，但是忽略了一点：<strong>一个用户</strong>在同一方面，对不同的物品的喜好程度或者要求是不一样的。（更有针对性！）</li>
<li>目前的方法都没有考虑到一个用户在同一物品的关注方面可能是多样的情况。<blockquote>
<p>例如，对于<strong>高品质手机</strong>，用户可能更关注于<strong>高像素、低耗量</strong>。而对于<strong>廉价手机</strong>，用户可能更关注于<strong>通讯的质量</strong>。</p>
</blockquote>
</li>
</ul>
<h2 id="2-提出了什么方法-有什么创新点"><a href="#2-提出了什么方法-有什么创新点" class="headerlink" title="2. 提出了什么方法 ? 有什么创新点"></a>2. 提出了什么方法 ? 有什么创新点</h2><p>A3NCF模型，一个新的主题模型以同时提取<strong>用户的偏好</strong>和物品的特征，可以准确地捕获<strong>用户对不同物品各个方面的关注</strong>。<br>它不同于之前的topic-based method直接利用LDA对评论进行提取项目特征。</p>
<ul>
<li><strong>aspect-level</strong>分成两个部分，一个是<strong>基于主题模型</strong>，一个是<strong>基于Attention network中隐层k</strong>。</li>
</ul>
<ol>
<li><strong>主题模型中</strong>将评论拆分为独立的句子，认为每个句子表示一个aspect信息，采用贝努利概率分布构建基于aspect的用户和物品的隐式主题向量。</li>
<li><strong>在神经网络端</strong>，作者参考了AFM模型，认为隐层K具有代表不同aspect信息的能力，采用attention的方法增强区分不同aspect的重要程度的能力。</li>
</ol>
<h2 id="3-原理"><a href="#3-原理" class="headerlink" title="3. 原理"></a>3. 原理</h2><p><img src="https://img-blog.csdnimg.cn/2019102913394761.png?" alt="在这里插入图片描述"><br>该模型主要分为四个模块</p>
<ul>
<li><strong>Input</strong></li>
<li><strong>Feature Fusion</strong></li>
<li><strong>Attentive Interaction</strong></li>
<li><strong>Rating Prediction</strong><h3 id="1-Input模块"><a href="#1-Input模块" class="headerlink" title="1. Input模块"></a>1. Input模块</h3>是由基于主题模型的用户和物品评论表示，以及基于<strong>one-hot编码</strong>的用户和物品的隐层表示作为输入。<br>在<strong>主题模型</strong>当中，作者对原来的<strong>LDA模型</strong>中引入了<strong>贝努利概率分布</strong>，<em>增强模型在用户和物品对不同aspect-level的topic的学习能力。</em><br><img src="https://img-blog.csdnimg.cn/20190524135731807.png?" alt="在这里插入图片描述"><h4 id="对应算法："><a href="#对应算法：" class="headerlink" title="对应算法："></a>对应算法：</h4><img src="https://img-blog.csdnimg.cn/20190524141219360.png" alt="在这里插入图片描述"><br>符号解释：<br>在图中，阴影圆圈表示观察到的变量w，而无阴影圆圈表示潜在变量。<br>M：用户数量<br>N：物品数量<br>D：语料库 （其中包含用户对项目的评论，d u，i∈D）<br>为了确定句子s的主题Zs，我们的模型引入了指标变量y∈{0,1}基于伯努利分布，该分布由π u。（因为对于不同的项目，用户可能会在不同的方面进行评论，这也反映了用户对不同项目的方面的关注。因此，πu是用户相关的，表示用户u倾向于根据自己的偏好或根据项目i的特征进行评论。）</li>
</ul>
<p>具体来说，当y = 0时，句子是从用户的偏好； 否则，它是根据项目的特征。<br><em>代码体现</em></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; K; k++) &#123;</span><br><span class="line">	<span class="comment">// common part</span></span><br><span class="line">	<span class="keyword">double</span> common_part = (nkt[k][doc[m][n]] + beta) / (nktSum[k] + betaSum);</span><br><span class="line">	<span class="comment">// first part is when y = 0; // USER</span></span><br><span class="line">	p[k] = (eta[<span class="number">0</span>] + Ny0[userIdx]) * common_part * ((nuk[userIdx][k] + alpha_u) / (nukSum[userIdx] + alphaSum));</span><br><span class="line"></span><br><span class="line">	<span class="comment">// second part is when y = 1 // ITEM</span></span><br><span class="line">	p[k + K] = (eta[<span class="number">1</span>] + Ny1[userIdx]) * common_part * (nvk[itemIdx][k] + alpha_v) / (nvkSum[itemIdx] + alphaSum);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>用于参数学习的不同推理方法主题模型的开发，例如变异推断和吉布斯采样<br>我们采用折叠的吉布斯抽样方法吉布斯抽样方法<br><em>代码体现</em></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Use Gibbs Sampling to update z[][]</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> m = <span class="number">0</span>; m &lt; M; m++) &#123;</span><br><span class="line">	<span class="keyword">int</span> N = docSet.docs.get(m).docWords.length;</span><br><span class="line">	<span class="keyword">int</span> userIdx = docSet.docs.get(m).userIdx;</span><br><span class="line">	<span class="keyword">int</span> itemIdx = docSet.docs.get(m).itemIdx;</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> n = <span class="number">0</span>; n &lt; N; n++) &#123;</span><br><span class="line">		sampling(userIdx, itemIdx, m, n);  <span class="comment">// 这个函数实现了吉布斯采样</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20200414000559240.png" alt="在这里插入图片描述"><br>来看看参数设计</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span>[][] doc;<span class="comment">// sent index array</span></span><br><span class="line"><span class="keyword">int</span> V, K, M;<span class="comment">// vocabulary size, topic number, document number 词汇量，主题编号，文件编号</span></span><br><span class="line"><span class="keyword">int</span> userNum, itemNum; <span class="comment">// number of users, number of items,</span></span><br><span class="line"><span class="keyword">int</span>[][] z;<span class="comment">// doc-term topic</span></span><br><span class="line"><span class="keyword">int</span>[][] y; <span class="comment">// doc-term y index</span></span><br><span class="line"><span class="keyword">float</span> alpha_u, alpha_v; <span class="comment">// doc-topic dirichlet prior parameter doc-topic dirichlet先验参数</span></span><br><span class="line"><span class="keyword">float</span> beta; <span class="comment">// topic-word dirichlet prior parameter 主题词dirichlet先验参数</span></span><br><span class="line"><span class="keyword">float</span>[] eta;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span>[][] nkt;<span class="comment">// given topic k, count times of term t. K*V 给定主题k，计算项t的次数。 K * V</span></span><br><span class="line"><span class="keyword">int</span>[] nktSum;<span class="comment">// Sum for each row in nkt</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span>[] Ny0; <span class="comment">// number of times sentence drawn from user 从用户抽取句子的次数</span></span><br><span class="line"><span class="keyword">int</span>[] Ny1; <span class="comment">// number of times sentence drawn from item 从物品抽取句子的次数</span></span><br><span class="line"><span class="keyword">double</span>[] NySum;</span><br><span class="line"><span class="keyword">double</span>[] bernpi;</span><br><span class="line"></span><br><span class="line"><span class="keyword">double</span>[][] phi;<span class="comment">// Parameters for topic-word distribution K*V 主题词分布的参数K * V</span></span><br><span class="line"><span class="keyword">double</span>[][] thetaU;<span class="comment">// Parameters for user-topic distribution 用户主题分布的参数</span></span><br><span class="line"><span class="keyword">double</span>[][] oldThetaU; <span class="comment">// user topic distribution</span></span><br><span class="line"><span class="keyword">double</span>[][] thetaV; <span class="comment">// parameters for item-topic distribution; 物品主题分布的参数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span>[][] nuk; <span class="comment">// number of times topic for user u</span></span><br><span class="line"><span class="keyword">int</span>[][] nvk; <span class="comment">// number of times topic for item v</span></span><br><span class="line"><span class="keyword">int</span>[] nukSum; <span class="comment">// userNumber</span></span><br><span class="line"><span class="keyword">int</span>[] nvkSum; <span class="comment">// itemNum</span></span><br></pre></td></tr></table></figure>

<h3 id="2-Feature-Fusion"><a href="#2-Feature-Fusion" class="headerlink" title="2. Feature Fusion"></a>2. Feature Fusion</h3><p>融合嵌入<strong>词向量特征</strong>和<strong>基于评论的特征</strong>。<br>融合的方式常用的有三种：级联、相加、元素相乘。本文使用<strong>相加</strong>融合。<br><em>代码体现</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">user_embed = user_id_embed + user_text <span class="comment"># 相加</span></span><br><span class="line">item_embed = item_id_embed + item_text <span class="comment"># 相加</span></span><br><span class="line">user_embed = self.user_fusion(user_embed)</span><br><span class="line">item_embed = self.item_fusion(item_embed)</span><br></pre></td></tr></table></figure>

<p>在融合后直接加了 一个<strong>全连接层</strong>，这一层采用了<strong>ReLU激活函数</strong>。实验证明，全连接层提高了性能效果。<br><em>代码体现</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">self.user_fusion = nn.Sequential(</span><br><span class="line">	nn.Linear(self.embed_size, self.embed_size),</span><br><span class="line">	nn.ReLU())</span><br><span class="line">self.item_fusion = nn.Sequential(</span><br><span class="line">	nn.Linear(self.embed_size, self.embed_size),</span><br><span class="line">	nn.ReLU())</span><br></pre></td></tr></table></figure>

<h3 id="3-Attentive-Interaction（最核心）"><a href="#3-Attentive-Interaction（最核心）" class="headerlink" title="3. Attentive Interaction（最核心）"></a>3. Attentive Interaction（最核心）</h3><h4 id="我们为了得到F："><a href="#我们为了得到F：" class="headerlink" title="我们为了得到F："></a>我们为了得到F：</h4><p><img src="https://img-blog.csdnimg.cn/20190524142236246.png" alt="在这里插入图片描述"><br>符号解释：</p>
<ul>
<li>pu代表用户向量，qi代表物品向量</li>
<li><strong>au,i是用户u对项目i的关注向量</strong></li>
<li>圈圈代表了逐个元素乘积 <blockquote>
<p>又称哈达玛积(Hadamard product)是矩阵的一类运算，若A=(aij)和B=(bij)是两个同阶矩阵，若cij=aij×bij,则称矩阵C=(cij)为A和B的哈达玛积<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9ia2ltZy5jZG4uYmNlYm9zLmNvbS9waWMvYjdmZDUyNjZkMDE2MDkyNGE4ZGY1OTI4ZDkwNzM1ZmFlNmNkMzQwZQ?x-oss-process=image/format,png" alt="在这里插入图片描述"></p>
</blockquote>
</li>
</ul>
<p><em>代码体现</em></p>
<h2 id="interact-att-weights-user-embed-item-embed"><a href="#interact-att-weights-user-embed-item-embed" class="headerlink" title="interact = att_weights * user_embed * item_embed"></a><code>interact = att_weights * user_embed * item_embed</code></h2><h4 id="怎么得到a"><a href="#怎么得到a" class="headerlink" title="怎么得到a"></a>怎么得到a</h4><p><img src="https://img-blog.csdnimg.cn/20190524143412884.png" alt="在这里插入图片描述"><br>符号解释：<br> θ u ： 用户u的特征方面的概率分布<br>ϕ i ： 项目i的特征方面的概率分布<br> vT： 将隐藏层投影到输出注意力的权重向量。<br>作者将<strong>原有的主题向量</strong>和<strong>融合向量</strong>联合生成<strong>attention权重</strong>，在实验中证明该操作比单纯用融合向量的性能要好。<br><em>代码体现</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">feature_all = torch.cat((</span><br><span class="line">			user_embed, item_embed), dim=<span class="number">-1</span>)</span><br><span class="line">att_weights = self.att_layer2(self.att_layer1(feature_all))</span><br><span class="line">att_weights = F.softmax(att_weights, dim=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">self.att_layer1 = nn.Sequential(</span><br><span class="line">	nn.Linear(<span class="number">2</span> * self.embed_size, <span class="number">1</span>),</span><br><span class="line">	nn.ReLU())</span><br><span class="line">self.att_layer2 = nn.Linear(<span class="number">1</span>, self.embed_size, bias=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20190524143448998.png" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/20200414083825799.png" alt="在这里插入图片描述"></p>
<h3 id="4-Rating-Prediction"><a href="#4-Rating-Prediction" class="headerlink" title="4. Rating Prediction"></a>4. Rating Prediction</h3><p><strong>F被送到全连接层</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prediction = self.rating_predict(interact)</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20190524143834934.png" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.rating_predict = nn.Sequential(</span><br><span class="line">	nn.Linear(self.embed_size, self.embed_size),</span><br><span class="line">	nn.ReLU(),</span><br><span class="line">	nn.Dropout(p=self.dropout),</span><br><span class="line">	nn.Linear(self.embed_size, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<p>通过回归层获得预测的评分<br><img src="https://img-blog.csdnimg.cn/20190524144027144.png" alt="在这里插入图片描述"></p>
<h2 id="4-损失函数-amp-优化器"><a href="#4-损失函数-amp-优化器" class="headerlink" title="4. 损失函数&amp;优化器"></a>4. 损失函数&amp;优化器</h2><h3 id="损失函数："><a href="#损失函数：" class="headerlink" title="损失函数："></a>损失函数：</h3><p><img src="https://img-blog.csdnimg.cn/20190411230123100.png" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.MSELoss()</span><br></pre></td></tr></table></figure>

<h3 id="optimization："><a href="#optimization：" class="headerlink" title="optimization："></a>optimization：</h3><p>SGD随机梯度下降</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(</span><br><span class="line">	model.parameters(), </span><br><span class="line">	lr=FLAGS.lr, weight_decay=FLAGS.decay)</span><br></pre></td></tr></table></figure>

<p>三种代码的默认都是Adam,不知道为啥。</p>
<h2 id="5-数据集是怎么划分的"><a href="#5-数据集是怎么划分的" class="headerlink" title="5. 数据集是怎么划分的?"></a>5. 数据集是怎么划分的?</h2><p>数据集中采用Amazon评论数据集和yelp评论数据集：<br><img src="https://img-blog.csdnimg.cn/2019041123014974.png?" alt="在这里插入图片描述"><br>类别：婴儿、杂货店和美食家、家里和厨房、庭院草坪和花园、运动和户外 </p>
<p>项目中数据集的结构<br><img src="https://img-blog.csdnimg.cn/20200407195544758.png" alt="在这里插入图片描述"><br>dat 三列，前两个分别代表第u个用户，第i个的物品，第列是评分。构成用户评分矩阵<br>包括了train和test。<br><img src="https://img-blog.csdnimg.cn/20200413232059701.png" alt="在这里插入图片描述"><br>theta 六列，有序，五列。似乎是对top个词   top5<img src="https://img-blog.csdnimg.cn/20200413232257435.png" alt="在这里插入图片描述"></p>
<h2 id="6-实验的结果："><a href="#6-实验的结果：" class="headerlink" title="6. 实验的结果："></a>6. 实验的结果：</h2><p>Baseline的对比：<br>（1） BMF: 经典<strong>基于评分</strong>的<strong>矩阵分解</strong>模型；<br>（2） HFT：联合<strong>MF</strong>和<strong>LDA评分</strong>预测模型；<br>（3） RMR：采用<strong>混合高斯模型</strong>预测评分；<br>（4） RBLT：MF与LDA <strong>线性组合</strong>预测模型；<br>（5） TransNet：采用基于<strong>CNN</strong>建模方法。<br>实验的结果：<br>总体RMSE的对比：<br><img src="https://img-blog.csdnimg.cn/20190411230227298.png?" alt="在这里插入图片描述"><br>没有讨论的部分！(缺点 &amp; 改进的地方)</p>
<hr>
<h2 id="疑问："><a href="#疑问：" class="headerlink" title="疑问："></a>疑问：</h2><h5 id="1-这里面打开为什么什么都没有呀？"><a href="#1-这里面打开为什么什么都没有呀？" class="headerlink" title="1. 这里面打开为什么什么都没有呀？"></a>1. 这里面打开为什么什么都没有呀？</h5><p><img src="https://img-blog.csdnimg.cn/20200413223448958.png" alt="在这里插入图片描述"></p>
<h5 id="2-具体的怎么运用到现在的模型当中"><a href="#2-具体的怎么运用到现在的模型当中" class="headerlink" title="2. 具体的怎么运用到现在的模型当中"></a>2. 具体的怎么运用到现在的模型当中</h5><hr>
<h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><p><a href="https://blog.csdn.net/qq_35709076/article/details/90516371" target="_blank" rel="noopener">https://blog.csdn.net/qq_35709076/article/details/90516371</a><br><a href="https://blog.csdn.net/qq_30843221/article/details/89222266" target="_blank" rel="noopener">https://blog.csdn.net/qq_30843221/article/details/89222266</a><br><a href="https://blog.csdn.net/yjk13703623757/article/details/77016867" target="_blank" rel="noopener">https://blog.csdn.net/yjk13703623757/article/details/77016867</a><br><a href="https://blog.csdn.net/LoseInVain/article/details/88363776" target="_blank" rel="noopener">https://blog.csdn.net/LoseInVain/article/details/88363776</a></p>
</div><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-meta">本文作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Rr-shan</a></span></div><div class="post-copyright-type"><span class="post-copyright-meta">本文链接: </span><span class="post-copyright-info"><a href="https://rr-shan.github.io/2020/04/14/srf之论文阅读A3NCF/">https://rr-shan.github.io/2020/04/14/srf之论文阅读A3NCF/</a></span></div><div class="post-copyright-notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://rr-shan.github.io">Sanzzi</a>！</span></div></div></article><div id="pagination"><div class="prev-post pull-left"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="../../20/srf之论文阅读Graph_embedding_review/"><i class="fas fa-angle-left">&nbsp;</i><span>srf之论文阅读Graph_embedding_review</span></a></div><div class="next-post pull-right"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="../../08/没能成功去泰迪杯的原因/"><span>没能成功去泰迪杯的原因</span><span>&nbsp;</span><i class="fas fa-angle-right"></i></a></div></div><!--div!= paginator()--></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fas fa-file-o"></i></span><span id="busuanzi_value_page_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2020 By Rr-shan</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="../../../../js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../../../js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../../../js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../../../js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../../../js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../../../js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../../../js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../../../js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../../../js/copy.js"></script><!--script(src=url)--><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/haruto.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body></html>