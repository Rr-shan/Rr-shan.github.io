<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="新浪微博首页爬虫小demo"><meta name="keywords" content><meta name="author" content="Rr-shan,undefined"><meta name="copyright" content="Rr-shan"><title>新浪微博首页爬虫小demo【Sanzzi】</title><link rel="stylesheet" href="/css/fan.css"><link rel="stylesheet" href="/css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="icon" href="/favicon.ico"><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="/js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
}</script></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="toggle-sidebar-info button-hover"><span data-toggle="文章目录">站点概览</span></div><div class="sidebar-toc"><div class="sidebar-toc-title">目录</div><div class="sidebar-toc-progress"><span class="progress-notice">您已阅读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc-progress-bar"></div></div><div class="sidebar-toc-content" id="sidebar-toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#模拟成浏览器"><span class="toc-number">1.</span> <span class="toc-text">模拟成浏览器</span></a></li></ol><li class="toc-item toc-level-2"><a class="toc-link" href="#淘宝爬图"><span class="toc-number"></span> <span class="toc-text">淘宝爬图</span></a></li></div></div><div class="author-info hide"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">Rr-shan</div><div class="author-info-description"></div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/Rr-shan" target="_blank">GitHub<i class="icon-dot bg-color6"></i></a><a class="links-button button-hover" href="mailto:1224559633@qq.com" target="_blank">E-Mail<i class="icon-dot bg-color2"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="/archives"><span class="pull-top">日志</span><span class="pull-bottom">22</span></a><a class="author-info-articles-tags article-meta" href="/tags"><span class="pull-top">标签</span><span class="pull-bottom">3</span></a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="title-name" href="/">Sanzzi</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><article id="post"><div class="post-header"><div class="title">新浪微博首页爬虫小demo</div><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 发表于 2019-12-04 | 更新于 2019-12-05</time><!--time.button-hover.post-date #[i.fas.fa-calendar-alt.article-icon(aria-hidden="true")] #[=__('post.modified')] #[=date(page['updated'], config.date_format)]--><div class="button-hover categories"></div><div class="button-hover tags"></div></div></div><div class="main-content"><p>HTTPError 是 URLError的子类<br>URLError</p>
<ol>
<li>连不上服务器</li>
<li>不存在</li>
<li>本地没有网络</li>
<li>HTTPError<br>使用URLError就完事了<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import urllib.error</span><br><span class="line">import urllib.request</span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">    urllib.request.urlopen(&quot;thhp://blog.csdn.net&quot;)</span><br><span class="line">except urllib.error.URLError as e:</span><br><span class="line">    if hasattr(e,&quot;code&quot;): # 判断是否有状态码</span><br><span class="line">        print(e.code)</span><br><span class="line">    if hasattr(e,&quot;reason&quot;): # 原因</span><br><span class="line">        print(e.reason)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="模拟成浏览器"><a href="#模拟成浏览器" class="headerlink" title="模拟成浏览器"></a>模拟成浏览器</h3><p>才被允许爬取，浏览器伪装一般通过报头进行 headers中的 user-agent 这个需要通过F12查找，可以刷新找到</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">url =&quot;https://i.csdn.net/#/uc/collection-list&quot;</span><br><span class="line">header=(&quot;User-Agent&quot;,&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36&quot;)</span><br><span class="line">opener=urllib.request.build_opener()</span><br><span class="line">opener.addheaders=[header]</span><br><span class="line">data=opener.open(url).read()</span><br><span class="line">fh=open(&quot;C:/Users/Administrator/Desktop/hello.html&quot;,&quot;wb&quot;) #html文件的打开方式地址的斜杠是相反的</span><br><span class="line">fh.write(data)</span><br><span class="line">fh.close()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line">import urllib.request</span><br><span class="line"></span><br><span class="line">##新浪新闻打不开，用网易新闻代替好了</span><br><span class="line">data=urllib.request.urlopen(&quot;https://news.sina.com.cn/&quot;).read()</span><br><span class="line">data1 = data.decode(&quot;gbk&quot;) # 这个设置为gbk居然可以了 ，这种问题在之后是常常遇到的</span><br><span class="line"></span><br><span class="line"># data1=data.decode(&quot;utf-8&quot;,&quot;ignore&quot;)</span><br><span class="line"># 忽视了的话出现的问题也很不友好！ “ҵʱ±Żվ&quot;”</span><br><span class="line"></span><br><span class="line"># Crrl+F键 可以查找</span><br><span class="line"></span><br><span class="line">pat = &apos;href=&quot;(https://news.sina.com.cn/.*?)&quot;&gt;&apos;</span><br><span class="line">allurl = re.compile(pat).findall(data1)</span><br><span class="line"># for i in allurl</span><br><span class="line"></span><br><span class="line">for i in range(0,len(allurl)):</span><br><span class="line">    try:</span><br><span class="line">        print(&quot;第&quot;+str(i)+&quot;次爬取&quot;)</span><br><span class="line">        thisurl=allurl[i]</span><br><span class="line">        file= &quot;C:/Users/Administrator/Desktop/爬虫/&quot; + str(i)+&quot;.html&quot;</span><br><span class="line">        urllib.request.urlretrieve(thisurl,file)</span><br><span class="line">    except urllib.error.URLError as e:</span><br><span class="line">        if hasattr(e,&quot;code&quot;):</span><br><span class="line">            print(e.code)</span><br><span class="line">        if hasattr(e,&quot;reason&quot;):</span><br><span class="line">            print(e.reason)</span><br><span class="line"># 遇到异常真的直接就崩掉了！</span><br><span class="line"># 这次循环出现问题会跳到下一次循环当中？！</span><br><span class="line"># 循环爬取文章！ 以及伪装浏览器 CSDN</span><br></pre></td></tr></table></figure>

<p>正则表达式的使用： 单引号和双引号的注意点</p>
<h2 id="淘宝爬图"><a href="#淘宝爬图" class="headerlink" title="淘宝爬图"></a>淘宝爬图</h2><p>已经爬不到了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import re</span><br><span class="line">keyname=&quot;半身裙&quot;</span><br><span class="line">key=urllib.request.quote(keyname) # 因为无法识别，进行编码操作</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">headers = (&quot;User-Agent&quot;,&quot;&quot;)</span><br><span class="line">opener=urllib.request.build_opener()</span><br><span class="line">opener.addheaders=[headers]</span><br><span class="line">urllib.request.install_opener(opener)</span><br><span class="line"># 不是网址有问题</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">for i in range(1,20): # 爬取0到100页?</span><br><span class="line">    url=&quot;https://s.taobao.com/list?spm=a21bo.2017.201867-links-0.18.5af911d9Vko7z7&amp;q=&quot;+key+&quot;&amp;cat=16&amp;style=grid&amp;seller_type=taobao&amp;bcoffset=0&amp;s=&quot;+str(i*60)</span><br><span class="line">    data=urllib.request.urlopen(url).read().decode(&quot;utf-8&quot;,&quot;ignore&quot;)</span><br><span class="line">    pat=&apos;pic_url&quot;:&quot;//(.*?)&quot;&apos; # 正则表达式还是有问题</span><br><span class="line">    imageurl=re.compile(pat).findall(data)</span><br><span class="line">    print(imageurl)</span><br></pre></td></tr></table></figure>

<p>如何分析url<br><a href="https://s.taobao.com/list?spm=a21bo.2017.201867-links-0.18.5af911d9Vko7z7&amp;q=%E5%8D%8A%E8%BA%AB%E8%A3%99&amp;cat=16&amp;style=grid&amp;seller_type=taobao&amp;bcoffset=0&amp;s=60" target="_blank" rel="noopener">https://s.taobao.com/list?spm=a21bo.2017.201867-links-0.18.5af911d9Vko7z7&amp;q=%E5%8D%8A%E8%BA%AB%E8%A3%99&amp;cat=16&amp;style=grid&amp;seller_type=taobao&amp;bcoffset=0&amp;s=60</a><br><a href="https://s.taobao.com/list?spm=a21bo.2017.201867-links-0.18.5af911d9Vko7z7&amp;q=%E5%8D%8A%E8%BA%AB%E8%A3%99&amp;cat=16&amp;style=grid&amp;seller_type=taobao&amp;bcoffset=0&amp;s=120" target="_blank" rel="noopener">https://s.taobao.com/list?spm=a21bo.2017.201867-links-0.18.5af911d9Vko7z7&amp;q=%E5%8D%8A%E8%BA%AB%E8%A3%99&amp;cat=16&amp;style=grid&amp;seller_type=taobao&amp;bcoffset=0&amp;s=120</a></p>
<p>q到&amp;cat前截止，都是可以变的，是搜索词<br>是为了找规律，以便从一到一百页<br>s !!! s+=60 第一页是0</p>
<p>抓包分析的目的是为了找到真实的js地址，没有在源码中，就一定隐藏在相应js中，最后没有别的尝试的时候才抓包<br>抓包工具 fiddler<br>需要手动配置代理<br>如何抓包 最后几分钟<br><a href="https://www.bilibili.com/video/av22571713?p=23" target="_blank" rel="noopener">https://www.bilibili.com/video/av22571713?p=23</a></p>
</div><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-meta">本文作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Rr-shan</a></span></div><div class="post-copyright-type"><span class="post-copyright-meta">本文链接: </span><span class="post-copyright-info"><a href="https://rr-shan.github.io/2019/12/04/新浪微博首页爬虫小demo/">https://rr-shan.github.io/2019/12/04/新浪微博首页爬虫小demo/</a></span></div><div class="post-copyright-notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://rr-shan.github.io">Sanzzi</a>！</span></div></div></article><div id="pagination"><div class="prev-post pull-left"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/2019/12/06/聚类的评估指标/"><i class="fas fa-angle-left">&nbsp;</i><span>聚类的评估指标</span></a></div><div class="next-post pull-right"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/2019/11/30/爬虫大体结构/"><span>爬虫大体结构</span><span>&nbsp;</span><i class="fas fa-angle-right"></i></a></div></div><!--div!= paginator()--></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fas fa-file-o"></i></span><span id="busuanzi_value_page_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2020 By Rr-shan</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/copy.js"></script><!--script(src=url)--><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/haruto.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body></html>