<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="机器学习初步"><meta name="keywords" content><meta name="author" content="Rr-shan,undefined"><meta name="copyright" content="Rr-shan"><title>机器学习初步【Sanzzi】</title><link rel="stylesheet" href="/css/fan.css"><link rel="stylesheet" href="/css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="icon" href="/favicon.ico"><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="/js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
}</script></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="toggle-sidebar-info button-hover"><span data-toggle="文章目录">站点概览</span></div><div class="sidebar-toc"><div class="sidebar-toc-title">目录</div><div class="sidebar-toc-progress"><span class="progress-notice">您已阅读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc-progress-bar"></div></div><div class="sidebar-toc-content" id="sidebar-toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#classification"><span class="toc-number">1.</span> <span class="toc-text">classification</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#regression"><span class="toc-number">2.</span> <span class="toc-text">regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#clustering-无监督学习中很重要的聚类"><span class="toc-number">3.</span> <span class="toc-text">clustering (无监督学习中很重要的聚类)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#demensionality-reduction（降维）"><span class="toc-number">4.</span> <span class="toc-text">demensionality reduction（降维）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#model-selection"><span class="toc-number">5.</span> <span class="toc-text">model selection</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#preprocessing（预处理）"><span class="toc-number">6.</span> <span class="toc-text">preprocessing（预处理）</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">Rr-shan</div><div class="author-info-description"></div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/Rr-shan" target="_blank">GitHub<i class="icon-dot bg-color6"></i></a><a class="links-button button-hover" href="mailto:1224559633@qq.com" target="_blank">E-Mail<i class="icon-dot bg-color2"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="/archives"><span class="pull-top">日志</span><span class="pull-bottom">25</span></a><a class="author-info-articles-tags article-meta" href="/tags"><span class="pull-top">标签</span><span class="pull-bottom">5</span></a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="title-name" href="/">Sanzzi</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><article id="post"><div class="post-header"><div class="title">机器学习初步</div><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 发表于 2019-10-24 | 更新于 2019-10-24</time><!--time.button-hover.post-date #[i.fas.fa-calendar-alt.article-icon(aria-hidden="true")] #[=__('post.modified')] #[=date(page['updated'], config.date_format)]--><div class="button-hover categories"></div><div class="button-hover tags"></div></div></div><div class="main-content"><p><text color="grey">人工智能和大数据关系不是那么大，甚至可以说二者的方向相差比较远！！！难怪会被嘲笑！！！</text></p>
<p>sklearn Python<br>数据量不大的话，单机用这个就可以实现了。// 基础</p>
<h2 id="classification"><a href="#classification" class="headerlink" title="classification"></a>classification</h2><ul>
<li>definationg:identifying to which category an object belongs to.</li>
<li>Applications:Spam detection,Image recognition</li>
<li>Algorithms:SVM、nearest neighbors、random forest</li>
</ul>
<h2 id="regression"><a href="#regression" class="headerlink" title="regression"></a>regression</h2><ul>
<li>defination:predicting a continuous-valued attribute associated with an object.</li>
<li>Application:Drug response,Stock prices</li>
<li>Agorithms:SVR、ridge regression、Lasso</li>
</ul>
<blockquote>
<p>岭回归，又称脊回归、吉洪诺夫正则化（Tikhonov regularization），是对不适定问题（ill-posed problem)进行回归分析时最经常使用的一种正则化方法。<br>岭回归(英文名：ridge regression, Tikhonov regularization)是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对病态数据的拟合要强于最小二乘法。</p>
</blockquote>
<blockquote>
<p>Lasso:该方法是一种压缩估计。它通过构造一个惩罚函数得到一个较为精炼的模型，使得它压缩一些回归系数，即强制系数绝对值之和小于某个固定值；同时设定一些回归系数为零。因此保留了子集收缩的优点，是一种处理具有复共线性数据的有偏估计。</p>
</blockquote>
<h2 id="clustering-无监督学习中很重要的聚类"><a href="#clustering-无监督学习中很重要的聚类" class="headerlink" title="clustering (无监督学习中很重要的聚类)"></a>clustering (无监督学习中很重要的聚类)</h2><ul>
<li>automatic grouping of similar objects into sets.</li>
<li>Application: customer segmentation 、 grouping experiment outcomes.</li>
<li>Agorithms:K-Means、spectral clustering、mean-shift</li>
</ul>
<blockquote>
<p>k均值聚类算法（k-means clustering algorithm）是一种迭代求解的聚类分析算法，其步骤是随机选取K个对象作为初始的聚类中心，然后计算每个对象与各个种子聚类中心之间的距离，把每个对象分配给距离它最近的聚类中心。聚类中心以及分配给它们的对象就代表一个聚类。每分配一个样本，聚类的聚类中心会根据聚类中现有的对象被重新计算。这个过程将不断重复直到满足某个终止条件。终止条件可以是没有（或最小数目）对象被重新分配给不同的聚类，没有（或最小数目）聚类中心再发生变化，误差平方和局部最小。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import random</span><br><span class="line">import sys</span><br><span class="line">import time</span><br><span class="line">class KMeansClusterer:</span><br><span class="line">    def __init__(self,ndarray,cluster_num):</span><br><span class="line">        self.ndarray = ndarray</span><br><span class="line">        self.cluster_num = cluster_num</span><br><span class="line">        self.points=self.__pick_start_point(ndarray,cluster_num)</span><br><span class="line">         </span><br><span class="line">    def cluster(self):</span><br><span class="line">        result = []</span><br><span class="line">        for i in range(self.cluster_num):</span><br><span class="line">            result.append([])</span><br><span class="line">        for item in self.ndarray:</span><br><span class="line">            distance_min = sys.maxsize</span><br><span class="line">            index=-1</span><br><span class="line">            for i in range(len(self.points)):                </span><br><span class="line">                distance = self.__distance(item,self.points[i])</span><br><span class="line">                if distance &lt; distance_min:</span><br><span class="line">                    distance_min = distance</span><br><span class="line">                    index = i</span><br><span class="line">            result[index] = result[index] + [item.tolist()]</span><br><span class="line">        new_center=[]</span><br><span class="line">        for item in result:</span><br><span class="line">            new_center.append(self.__center(item).tolist())</span><br><span class="line">        # 中心点未改变，说明达到稳态，结束递归</span><br><span class="line">        if (self.points==new_center).all():</span><br><span class="line">            return result</span><br><span class="line">         </span><br><span class="line">        self.points=np.array(new_center)</span><br><span class="line">        return self.cluster()</span><br><span class="line">             </span><br><span class="line">    def __center(self,list):</span><br><span class="line">        &apos;&apos;&apos;计算一组坐标的中心点</span><br><span class="line">        &apos;&apos;&apos;</span><br><span class="line">        # 计算每一列的平均值</span><br><span class="line">        return np.array(list).mean(axis=0)</span><br><span class="line">    def __distance(self,p1,p2):</span><br><span class="line">        &apos;&apos;&apos;计算两点间距</span><br><span class="line">        &apos;&apos;&apos;</span><br><span class="line">        tmp=0</span><br><span class="line">        for i in range(len(p1)):</span><br><span class="line">            tmp += pow(p1[i]-p2[i],2)</span><br><span class="line">        return pow(tmp,0.5)</span><br><span class="line">    def __pick_start_point(self,ndarray,cluster_num):</span><br><span class="line">        </span><br><span class="line">        if cluster_num &lt;0 or cluster_num &gt; ndarray.shape[0]:</span><br><span class="line">            raise Exception(&quot;簇数设置有误&quot;)</span><br><span class="line">      </span><br><span class="line">        # 随机点的下标</span><br><span class="line">        indexes=random.sample(np.arange(0,ndarray.shape[0],step=1).tolist(),cluster_num)</span><br><span class="line">        points=[]</span><br><span class="line">        for index in indexes:</span><br><span class="line">            points.append(ndarray[index].tolist())</span><br><span class="line">        return np.array(points)</span><br></pre></td></tr></table></figure>

<h2 id="demensionality-reduction（降维）"><a href="#demensionality-reduction（降维）" class="headerlink" title="demensionality reduction（降维）"></a>demensionality reduction（降维）</h2><ul>
<li>defination:reducing the number of random variables to consider. 方便计算</li>
<li>Appication: visualization (看着舒服))、 increased efficiency</li>
<li>Algorithms:PCA、feature selection 、 non-negative matrix factorization</li>
</ul>
<h2 id="model-selection"><a href="#model-selection" class="headerlink" title="model selection"></a>model selection</h2><ul>
<li>comparing、validating（验证） and choosing parameters and models.</li>
<li>Goal: improved accuracy via parameter tuning</li>
<li>Modules: grid search（调参手段）、cross validation（交叉验证）、metrics（度量，指标）</li>
</ul>
<h2 id="preprocessing（预处理）"><a href="#preprocessing（预处理）" class="headerlink" title="preprocessing（预处理）"></a>preprocessing（预处理）</h2><ul>
<li>feaure extraction(特征抽取) and normalization(归一化))</li>
<li>Application:transforming input data such as text for use with machine learning algorithms.</li>
</ul>
</div><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-meta">本文作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Rr-shan</a></span></div><div class="post-copyright-type"><span class="post-copyright-meta">本文链接: </span><span class="post-copyright-info"><a href="https://rr-shan.github.io/2019/10/24/机器学习初步/">https://rr-shan.github.io/2019/10/24/机器学习初步/</a></span></div><div class="post-copyright-notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://rr-shan.github.io">Sanzzi</a>！</span></div></div></article><div id="pagination"><div class="prev-post pull-left"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/2019/11/09/大数据学习的基本框架/"><i class="fas fa-angle-left">&nbsp;</i><span>大数据学习的基本框架</span></a></div><div class="next-post pull-right"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/2019/10/23/排序算法/"><span>排序算法（C++实现）</span><span>&nbsp;</span><i class="fas fa-angle-right"></i></a></div></div><!--div!= paginator()--></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fas fa-file-o"></i></span><span id="busuanzi_value_page_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2019 By Rr-shan</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/copy.js"></script><!--script(src=url)--><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/haruto.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body></html>